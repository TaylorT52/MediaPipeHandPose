{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mediapipe in /Users/taylor/Library/Python/3.9/lib/python/site-packages (0.9.1.0)\n",
      "Requirement already satisfied: opencv-python in /Users/taylor/Library/Python/3.9/lib/python/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: absl-py in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from mediapipe) (21.4.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from mediapipe) (23.5.26)\n",
      "Requirement already satisfied: matplotlib in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from mediapipe) (3.8.2)\n",
      "Requirement already satisfied: numpy in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from mediapipe) (1.26.3)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from mediapipe) (4.9.0.80)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from mediapipe) (3.20.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (6.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib->mediapipe) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Frameworks/Python.framework/Versions/3.9/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ultralytics in /Users/taylor/Library/Python/3.9/lib/python/site-packages (8.1.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from ultralytics) (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.22.2 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from ultralytics) (1.26.3)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from ultralytics) (4.9.0.80)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from ultralytics) (10.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from ultralytics) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from ultralytics) (1.12.0)\n",
      "Requirement already satisfied: torch>=1.8.0 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from ultralytics) (2.1.2)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from ultralytics) (0.16.2)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from ultralytics) (4.66.1)\n",
      "Requirement already satisfied: psutil in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from ultralytics) (5.9.7)\n",
      "Requirement already satisfied: py-cpuinfo in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: thop>=0.1.1 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from ultralytics) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from ultralytics) (2.1.4)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from ultralytics) (0.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from matplotlib>=3.3.0->ultralytics) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from matplotlib>=3.3.0->ultralytics) (6.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from requests>=2.23.0->ultralytics) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from requests>=2.23.0->ultralytics) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from requests>=2.23.0->ultralytics) (2023.11.17)\n",
      "Requirement already satisfied: filelock in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from torch>=1.8.0->ultralytics) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from torch>=1.8.0->ultralytics) (2023.12.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib>=3.3.0->ultralytics) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Frameworks/Python.framework/Versions/3.9/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pybboxes in /Users/taylor/Library/Python/3.9/lib/python/site-packages (0.1.6)\n",
      "Requirement already satisfied: numpy in /Users/taylor/Library/Python/3.9/lib/python/site-packages (from pybboxes) (1.26.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Frameworks/Python.framework/Versions/3.9/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe opencv-python\n",
    "!pip install ultralytics\n",
    "!pip install pybboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "import os\n",
    "from ultralytics.utils.plotting import Annotator "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load YOLO stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_loc = \"/Users/taylor/runs/detect/train14/weights/best.pt\"\n",
    "yolo_model = YOLO(weights_loc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Hand poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = 30\n",
    "img_counter = 0\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "desired_aspect_ratio = 1  # 1:1 ratio\n",
    "standard_size = (350, 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    lower_pink = np.array([140, 100, 100])\n",
    "    upper_pink = np.array([170, 255, 255])\n",
    "    mask = cv2.inRange(hsv_img, lower_pink, upper_pink)\n",
    "    result = cv2.bitwise_and(img, img, mask=mask)\n",
    "    return result\n",
    "\n",
    "def match_gestures(img1, img2, threshold=120):\n",
    "    img1_processed = preprocess_image(img1)\n",
    "    img2_processed = preprocess_image(img2)\n",
    "\n",
    "    orb = cv2.ORB_create()\n",
    "    kp1, des1 = orb.detectAndCompute(img1_processed, None)\n",
    "    kp2, des2 = orb.detectAndCompute(img2_processed, None)\n",
    "\n",
    "    if des1 is not None and des2 is not None and len(des1) > 0 and len(des2) > 0:\n",
    "        # Convert descriptors to type np.uint8 if they're not already\n",
    "        if des1.dtype != np.uint8:\n",
    "            des1 = des1.astype(np.uint8)\n",
    "        if des2.dtype != np.uint8:\n",
    "            des2 = des2.astype(np.uint8)\n",
    "\n",
    "        # Initialize the BFMatcher and perform matching\n",
    "        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "        matches = bf.match(des1, des2)\n",
    "        matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "        # Check if the number of matches exceeds the threshold\n",
    "        if len(matches) > threshold:\n",
    "            print(\"The gestures are similar.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"The gestures are not similar.\")\n",
    "            return False\n",
    "    else:\n",
    "        # Handle the case where one or both sets of descriptors are None or empty\n",
    "        print(\"One or both sets of descriptors are missing or empty.\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-03 14:23:07.132 Python[49339:458811] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 myright, 263.8ms\n",
      "Speed: 6.4ms preprocess, 263.8ms inference, 17.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 76.0ms\n",
      "Speed: 2.7ms preprocess, 76.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 90.3ms\n",
      "Speed: 2.0ms preprocess, 90.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 111.6ms\n",
      "Speed: 6.7ms preprocess, 111.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 104.1ms\n",
      "Speed: 1.7ms preprocess, 104.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 148.4ms\n",
      "Speed: 1.6ms preprocess, 148.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 114.7ms\n",
      "Speed: 1.8ms preprocess, 114.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 1 yourleft, 114.2ms\n",
      "Speed: 1.4ms preprocess, 114.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 111.7ms\n",
      "Speed: 6.2ms preprocess, 111.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 137.7ms\n",
      "Speed: 2.7ms preprocess, 137.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 90.8ms\n",
      "Speed: 1.6ms preprocess, 90.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 144.1ms\n",
      "Speed: 22.6ms preprocess, 144.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 195.4ms\n",
      "Speed: 1.5ms preprocess, 195.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 87.5ms\n",
      "Speed: 1.6ms preprocess, 87.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 95.5ms\n",
      "Speed: 1.7ms preprocess, 95.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 113.7ms\n",
      "Speed: 1.6ms preprocess, 113.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 120.0ms\n",
      "Speed: 1.6ms preprocess, 120.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 131.6ms\n",
      "Speed: 2.5ms preprocess, 131.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 (no detections), 124.2ms\n",
      "Speed: 1.5ms preprocess, 124.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 99.9ms\n",
      "Speed: 1.6ms preprocess, 99.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 95.5ms\n",
      "Speed: 1.4ms preprocess, 95.5ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 95.3ms\n",
      "Speed: 2.7ms preprocess, 95.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 92.6ms\n",
      "Speed: 1.4ms preprocess, 92.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 99.8ms\n",
      "Speed: 1.7ms preprocess, 99.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 myrights, 121.0ms\n",
      "Speed: 2.0ms preprocess, 121.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are similar.\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 106.3ms\n",
      "Speed: 1.8ms preprocess, 106.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are not similar.\n",
      "\n",
      "0: 384x640 1 myright, 97.6ms\n",
      "Speed: 1.6ms preprocess, 97.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 92.4ms\n",
      "Speed: 1.4ms preprocess, 92.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 114.6ms\n",
      "Speed: 1.4ms preprocess, 114.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 107.2ms\n",
      "Speed: 1.4ms preprocess, 107.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 134.3ms\n",
      "Speed: 1.5ms preprocess, 134.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 105.8ms\n",
      "Speed: 1.4ms preprocess, 105.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 93.6ms\n",
      "Speed: 1.3ms preprocess, 93.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 112.7ms\n",
      "Speed: 1.8ms preprocess, 112.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 98.8ms\n",
      "Speed: 1.4ms preprocess, 98.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 95.9ms\n",
      "Speed: 1.5ms preprocess, 95.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 97.8ms\n",
      "Speed: 1.4ms preprocess, 97.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 95.9ms\n",
      "Speed: 1.4ms preprocess, 95.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 97.0ms\n",
      "Speed: 1.5ms preprocess, 97.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 92.8ms\n",
      "Speed: 1.5ms preprocess, 92.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are not similar.\n",
      "\n",
      "0: 384x640 1 myright, 93.8ms\n",
      "Speed: 1.4ms preprocess, 93.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 105.5ms\n",
      "Speed: 1.4ms preprocess, 105.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 102.0ms\n",
      "Speed: 1.7ms preprocess, 102.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 98.0ms\n",
      "Speed: 1.6ms preprocess, 98.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 93.9ms\n",
      "Speed: 1.6ms preprocess, 93.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 95.8ms\n",
      "Speed: 1.4ms preprocess, 95.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 96.7ms\n",
      "Speed: 1.4ms preprocess, 96.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 108.4ms\n",
      "Speed: 1.7ms preprocess, 108.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 118.1ms\n",
      "Speed: 1.5ms preprocess, 118.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 119.2ms\n",
      "Speed: 1.5ms preprocess, 119.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 101.5ms\n",
      "Speed: 1.4ms preprocess, 101.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 (no detections), 104.8ms\n",
      "Speed: 1.4ms preprocess, 104.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 myright, 1 yourleft, 104.2ms\n",
      "Speed: 1.6ms preprocess, 104.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are not similar.\n",
      "The gestures are not similar.\n",
      "\n",
      "0: 384x640 1 yourleft, 98.6ms\n",
      "Speed: 1.7ms preprocess, 98.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are not similar.\n",
      "\n",
      "0: 384x640 1 myright, 1 yourleft, 110.4ms\n",
      "Speed: 1.4ms preprocess, 110.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are not similar.\n",
      "The gestures are not similar.\n",
      "\n",
      "0: 384x640 1 myright, 104.6ms\n",
      "Speed: 1.7ms preprocess, 104.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are not similar.\n",
      "\n",
      "0: 384x640 1 myright, 98.8ms\n",
      "Speed: 1.6ms preprocess, 98.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are not similar.\n",
      "\n",
      "0: 384x640 1 myright, 100.0ms\n",
      "Speed: 2.1ms preprocess, 100.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are not similar.\n",
      "\n",
      "0: 384x640 (no detections), 90.3ms\n",
      "Speed: 1.4ms preprocess, 90.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 91.0ms\n",
      "Speed: 1.5ms preprocess, 91.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 159.6ms\n",
      "Speed: 1.8ms preprocess, 159.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 95.6ms\n",
      "Speed: 1.4ms preprocess, 95.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 myright, 101.4ms\n",
      "Speed: 1.4ms preprocess, 101.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are not similar.\n",
      "\n",
      "0: 384x640 1 yourleft, 106.9ms\n",
      "Speed: 1.4ms preprocess, 106.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are not similar.\n",
      "\n",
      "0: 384x640 1 myright, 1 yourleft, 101.6ms\n",
      "Speed: 1.6ms preprocess, 101.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are not similar.\n",
      "The gestures are not similar.\n",
      "\n",
      "0: 384x640 1 myright, 110.2ms\n",
      "Speed: 1.6ms preprocess, 110.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are not similar.\n",
      "\n",
      "0: 384x640 1 myright, 96.2ms\n",
      "Speed: 1.3ms preprocess, 96.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are not similar.\n",
      "\n",
      "0: 384x640 2 myrights, 105.1ms\n",
      "Speed: 1.4ms preprocess, 105.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are not similar.\n",
      "The gestures are not similar.\n",
      "\n",
      "0: 384x640 1 myright, 103.7ms\n",
      "Speed: 1.8ms preprocess, 103.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "The gestures are not similar.\n",
      "\n",
      "0: 384x640 (no detections), 124.7ms\n",
      "Speed: 4.2ms preprocess, 124.7ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 myright, 94.4ms\n",
      "Speed: 1.8ms preprocess, 94.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 (no detections), 100.1ms\n",
      "Speed: 3.2ms preprocess, 100.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 114.9ms\n",
      "Speed: 2.1ms preprocess, 114.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 116.9ms\n",
      "Speed: 1.5ms preprocess, 116.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 120.5ms\n",
      "Speed: 1.5ms preprocess, 120.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 100.3ms\n",
      "Speed: 1.6ms preprocess, 100.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 103.4ms\n",
      "Speed: 1.9ms preprocess, 103.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 118.6ms\n",
      "Speed: 1.7ms preprocess, 118.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 100.1ms\n",
      "Speed: 1.6ms preprocess, 100.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 104.0ms\n",
      "Speed: 1.8ms preprocess, 104.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 104.9ms\n",
      "Speed: 1.9ms preprocess, 104.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 94.6ms\n",
      "Speed: 1.7ms preprocess, 94.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 103.0ms\n",
      "Speed: 1.5ms preprocess, 103.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 106.6ms\n",
      "Speed: 3.4ms preprocess, 106.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 106.3ms\n",
      "Speed: 1.5ms preprocess, 106.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 144.5ms\n",
      "Speed: 2.7ms preprocess, 144.5ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 108.4ms\n",
      "Speed: 1.9ms preprocess, 108.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 myright, 87.7ms\n",
      "Speed: 1.4ms preprocess, 87.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 135.6ms\n",
      "Speed: 1.5ms preprocess, 135.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 85.6ms\n",
      "Speed: 1.6ms preprocess, 85.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 (no detections), 123.3ms\n",
      "Speed: 3.2ms preprocess, 123.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 114.6ms\n",
      "Speed: 3.4ms preprocess, 114.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 myright, 117.1ms\n",
      "Speed: 1.6ms preprocess, 117.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 (no detections), 137.9ms\n",
      "Speed: 2.0ms preprocess, 137.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 myright, 134.0ms\n",
      "Speed: 1.6ms preprocess, 134.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 106.3ms\n",
      "Speed: 4.9ms preprocess, 106.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m width \u001b[39m=\u001b[39m canvas_for_yolo\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m     30\u001b[0m \u001b[39m#perform YOLO predictions\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m yolo_results \u001b[39m=\u001b[39m yolo_model\u001b[39m.\u001b[39;49mpredict(frame)\n\u001b[1;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m yolo_results:\n\u001b[1;32m     33\u001b[0m     \u001b[39m#annotate boxes\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     annotator \u001b[39m=\u001b[39m Annotator(canvas)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ultralytics/engine/model.py:275\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m prompts \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor, \u001b[39m\"\u001b[39m\u001b[39mset_prompts\u001b[39m\u001b[39m\"\u001b[39m):  \u001b[39m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 275\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39mpredict_cli(source\u001b[39m=\u001b[39msource) \u001b[39mif\u001b[39;00m is_cli \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictor(source\u001b[39m=\u001b[39;49msource, stream\u001b[39m=\u001b[39;49mstream)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ultralytics/engine/predictor.py:204\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream_inference(source, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    203\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream_inference(source, model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[39m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     37\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[39m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ultralytics/engine/predictor.py:283\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39m# Inference\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[39mwith\u001b[39;00m profilers[\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> 283\u001b[0m     preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minference(im, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    284\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39membed:\n\u001b[1;32m    285\u001b[0m         \u001b[39myield from\u001b[39;00m [preds] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(preds, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m preds  \u001b[39m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ultralytics/engine/predictor.py:140\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    135\u001b[0m visualize \u001b[39m=\u001b[39m (\n\u001b[1;32m    136\u001b[0m     increment_path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_dir \u001b[39m/\u001b[39m Path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mstem, mkdir\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    137\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mvisualize \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource_type\u001b[39m.\u001b[39mtensor)\n\u001b[1;32m    138\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    139\u001b[0m )\n\u001b[0;32m--> 140\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(im, augment\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49maugment, visualize\u001b[39m=\u001b[39;49mvisualize, embed\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49membed, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ultralytics/nn/autobackend.py:384\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[1;32m    381\u001b[0m     im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpt \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnn_module:  \u001b[39m# PyTorch\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(im, augment\u001b[39m=\u001b[39;49maugment, visualize\u001b[39m=\u001b[39;49mvisualize, embed\u001b[39m=\u001b[39;49membed)\n\u001b[1;32m    385\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjit:  \u001b[39m# TorchScript\u001b[39;00m\n\u001b[1;32m    386\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(im)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ultralytics/nn/tasks.py:80\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mdict\u001b[39m):  \u001b[39m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 80\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ultralytics/nn/tasks.py:98\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m augment:\n\u001b[1;32m     97\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict_augment(x)\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_once(x, profile, visualize, embed)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ultralytics/nn/tasks.py:119\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m profile:\n\u001b[1;32m    118\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 119\u001b[0m x \u001b[39m=\u001b[39m m(x)  \u001b[39m# run\u001b[39;00m\n\u001b[1;32m    120\u001b[0m y\u001b[39m.\u001b[39mappend(x \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mi \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)  \u001b[39m# save output\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ultralytics/nn/modules/block.py:221\u001b[0m, in \u001b[0;36mC2f.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv1(x)\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m--> 221\u001b[0m y\u001b[39m.\u001b[39;49mextend(m(y[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]) \u001b[39mfor\u001b[39;49;00m m \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mm)\n\u001b[1;32m    222\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv2(torch\u001b[39m.\u001b[39mcat(y, \u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ultralytics/nn/modules/block.py:221\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv1(x)\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m--> 221\u001b[0m y\u001b[39m.\u001b[39mextend(m(y[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]) \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mm)\n\u001b[1;32m    222\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv2(torch\u001b[39m.\u001b[39mcat(y, \u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ultralytics/nn/modules/block.py:331\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    330\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"'forward()' applies the YOLO FPN to input data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m     \u001b[39mreturn\u001b[39;00m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcv1(x)) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv1(x))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ultralytics/nn/modules/conv.py:54\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_fuse\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     53\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0) \n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  \n",
    "\n",
    "        #image loading\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.flip(image, 1)\n",
    "        results = hands.process(image)\n",
    "        canvas_for_yolo = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        h, w, c = frame.shape\n",
    "\n",
    "        #for drawing\n",
    "        canvas = np.zeros_like(image)\n",
    "\n",
    "        #draw results on black canvas\n",
    "        if results.multi_hand_landmarks:\n",
    "            for handLMs in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    canvas, handLMs, mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                    mp_drawing.DrawingSpec(color=(250, 44, 250), thickness=2, circle_radius=2)\n",
    "                )\n",
    "\n",
    "        width = canvas_for_yolo.shape[1]\n",
    "\n",
    "        #perform YOLO predictions\n",
    "        yolo_results = yolo_model.predict(frame)\n",
    "        for r in yolo_results:\n",
    "            #annotate boxes\n",
    "            annotator = Annotator(canvas)\n",
    "            boxes = r.boxes\n",
    "            for box in boxes:\n",
    "                b = box.xyxy[0]\n",
    "                mirrored_x_min = width - b[2]\n",
    "                mirrored_x_max = width - b[0]\n",
    "                mirrored_box = [mirrored_x_min-padding, b[1]-padding, mirrored_x_max+padding, b[3]+padding]\n",
    "                c = box.cls\n",
    "                #annotator.box_label(mirrored_box, \"hand\")\n",
    "                \n",
    "                #create cropped canvas for saving\n",
    "                save_me = canvas[max(int(mirrored_box[1]), 0):min(int(mirrored_box[3]), canvas.shape[0]),\n",
    "                               max(int(mirrored_box[0]), 0):min(int(mirrored_box[2]), canvas.shape[1])]\n",
    "\n",
    "\n",
    "                h, w = save_me.shape[:2]\n",
    "                current_aspect_ratio = w / h\n",
    "        \n",
    "                # Calculate padding\n",
    "                if current_aspect_ratio < desired_aspect_ratio:\n",
    "                    # Pad sides\n",
    "                    new_width = int(desired_aspect_ratio * h)\n",
    "                    pad_width = (new_width - w) // 2\n",
    "                    padded_image = cv2.copyMakeBorder(save_me, 0, 0, pad_width, pad_width, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "                else:\n",
    "                    # Pad top and bottom\n",
    "                    new_height = int(w / desired_aspect_ratio)\n",
    "                    pad_height = (new_height - h) // 2\n",
    "                    padded_image = cv2.copyMakeBorder(save_me, pad_height, pad_height, 0, 0, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "        \n",
    "                # Resize image to standard size\n",
    "                resized_image = cv2.resize(padded_image, standard_size, interpolation=cv2.INTER_AREA)\n",
    "                base_img  = cv2.imread(\"base_gestures/start_base.png\")\n",
    "                start = match_gestures(base_img, resized_image)\n",
    "                \n",
    "                if start:\n",
    "                    cv2.putText(canvas, \"start\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "                if cv2.waitKey(10) & 0xFF == ord('s'):\n",
    "                    img_name = f\"cropped_hand_{img_counter}.png\"\n",
    "                    cv2.imwrite(\"saved_imgs/\" + img_name, resized_image)\n",
    "                    print(f\"{img_name} saved.\")\n",
    "                    img_counter += 1\n",
    "\n",
    "        #display\n",
    "        cv2.imshow('Hand Skeleton', canvas)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
