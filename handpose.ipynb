{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement mediapipe (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for mediapipe\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting ultralytics\n",
      "  Downloading ultralytics-8.1.37-py3-none-any.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m681.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:--:--\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib>=3.3.0 (from ultralytics)\n",
      "  Using cached matplotlib-3.8.3-cp312-cp312-macosx_10_12_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting opencv-python>=4.6.0 (from ultralytics)\n",
      "  Using cached opencv_python-4.9.0.80-cp37-abi3-macosx_10_16_x86_64.whl.metadata (20 kB)\n",
      "Collecting pillow>=7.1.2 (from ultralytics)\n",
      "  Using cached pillow-10.2.0-cp312-cp312-macosx_10_10_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting pyyaml>=5.3.1 (from ultralytics)\n",
      "  Downloading PyYAML-6.0.1-cp312-cp312-macosx_10_9_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting requests>=2.23.0 (from ultralytics)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting scipy>=1.4.1 (from ultralytics)\n",
      "  Downloading scipy-1.12.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch>=1.8.0 (from ultralytics)\n",
      "  Downloading torch-2.2.2-cp312-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting torchvision>=0.9.0 (from ultralytics)\n",
      "  Downloading torchvision-0.17.2-cp312-cp312-macosx_10_13_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting tqdm>=4.64.0 (from ultralytics)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting psutil (from ultralytics)\n",
      "  Using cached psutil-5.9.8-cp36-abi3-macosx_10_9_x86_64.whl.metadata (21 kB)\n",
      "Collecting py-cpuinfo (from ultralytics)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting thop>=0.1.1 (from ultralytics)\n",
      "  Using cached thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting pandas>=1.1.4 (from ultralytics)\n",
      "  Downloading pandas-2.2.1-cp312-cp312-macosx_10_9_x86_64.whl.metadata (19 kB)\n",
      "Collecting seaborn>=0.11.0 (from ultralytics)\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Using cached contourpy-1.2.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Using cached fonttools-4.50.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (159 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Using cached kiwisolver-1.4.5-cp312-cp312-macosx_10_9_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting numpy<2,>=1.21 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_10_9_x86_64.whl.metadata (61 kB)\n",
      "Collecting packaging>=20.0 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Using cached packaging-24.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting python-dateutil>=2.7 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.1.4->ultralytics)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.1.4->ultralytics)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.23.0->ultralytics)\n",
      "  Downloading charset_normalizer-3.3.2-cp312-cp312-macosx_10_9_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.23.0->ultralytics)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.23.0->ultralytics)\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.23.0->ultralytics)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting filelock (from torch>=1.8.0->ultralytics)\n",
      "  Downloading filelock-3.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy (from torch>=1.8.0->ultralytics)\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.8.0->ultralytics)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting fsspec (from torch>=1.8.0->ultralytics)\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.8.0->ultralytics)\n",
      "  Downloading MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.8.0->ultralytics)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading ultralytics-8.1.37-py3-none-any.whl (723 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m723.1/723.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached matplotlib-3.8.3-cp312-cp312-macosx_10_12_x86_64.whl (7.6 MB)\n",
      "Using cached opencv_python-4.9.0.80-cp37-abi3-macosx_10_16_x86_64.whl (55.7 MB)\n",
      "Downloading pandas-2.2.1-cp312-cp312-macosx_10_9_x86_64.whl (12.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pillow-10.2.0-cp312-cp312-macosx_10_10_x86_64.whl (3.5 MB)\n",
      "Downloading PyYAML-6.0.1-cp312-cp312-macosx_10_9_x86_64.whl (178 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Downloading scipy-1.12.0-cp312-cp312-macosx_10_9_x86_64.whl (38.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.9/38.9 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Downloading torch-2.2.2-cp312-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.17.2-cp312-cp312-macosx_10_13_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached psutil-5.9.8-cp36-abi3-macosx_10_9_x86_64.whl (248 kB)\n",
      "Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Downloading charset_normalizer-3.3.2-cp312-cp312-macosx_10_9_x86_64.whl (122 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.2/122.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached contourpy-1.2.0-cp312-cp312-macosx_10_9_x86_64.whl (259 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.50.0-cp312-cp312-macosx_10_9_x86_64.whl (2.3 MB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached kiwisolver-1.4.5-cp312-cp312-macosx_10_9_x86_64.whl (67 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-macosx_10_9_x86_64.whl (20.3 MB)\n",
      "Using cached packaging-24.0-py3-none-any.whl (53 kB)\n",
      "Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.13.3-py3-none-any.whl (11 kB)\n",
      "Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Downloading MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_x86_64.whl (14 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytz, py-cpuinfo, mpmath, urllib3, tzdata, typing-extensions, tqdm, sympy, six, pyyaml, pyparsing, psutil, pillow, packaging, numpy, networkx, MarkupSafe, kiwisolver, idna, fsspec, fonttools, filelock, cycler, charset-normalizer, certifi, scipy, requests, python-dateutil, opencv-python, jinja2, contourpy, torch, pandas, matplotlib, torchvision, thop, seaborn, ultralytics\n",
      "Successfully installed MarkupSafe-2.1.5 certifi-2024.2.2 charset-normalizer-3.3.2 contourpy-1.2.0 cycler-0.12.1 filelock-3.13.3 fonttools-4.50.0 fsspec-2024.3.1 idna-3.6 jinja2-3.1.3 kiwisolver-1.4.5 matplotlib-3.8.3 mpmath-1.3.0 networkx-3.2.1 numpy-1.26.4 opencv-python-4.9.0.80 packaging-24.0 pandas-2.2.1 pillow-10.2.0 psutil-5.9.8 py-cpuinfo-9.0.0 pyparsing-3.1.2 python-dateutil-2.9.0.post0 pytz-2024.1 pyyaml-6.0.1 requests-2.31.0 scipy-1.12.0 seaborn-0.13.2 six-1.16.0 sympy-1.12 thop-0.1.1.post2209072238 torch-2.2.2 torchvision-0.17.2 tqdm-4.66.2 typing-extensions-4.10.0 tzdata-2024.1 ultralytics-8.1.37 urllib3-2.2.1\n",
      "Collecting pybboxes\n",
      "  Using cached pybboxes-0.1.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pybboxes) (1.26.4)\n",
      "Using cached pybboxes-0.1.6-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: pybboxes\n",
      "Successfully installed pybboxes-0.1.6\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe opencv-python\n",
    "!pip install ultralytics\n",
    "!pip install pybboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "import os\n",
    "from ultralytics.utils.plotting import Annotator \n",
    "import json\n",
    "from google.protobuf.json_format import MessageToDict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load YOLO stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_loc = \"weights/best.pt\"\n",
    "yolo_model = YOLO(weights_loc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Hand poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = 30\n",
    "img_counter = 0\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "desired_aspect_ratio = 1 \n",
    "standard_size = (350, 350)\n",
    "handedness = \"\"\n",
    "gestures = [\"speed_inc\", \"speed_dec\", \"to_right\", \"to_left\"]\n",
    "\n",
    "with open(\"base_gestures.json\", \"r\") as infile:\n",
    "    data = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    lower_pink = np.array([140, 100, 100])\n",
    "    upper_pink = np.array([170, 255, 255])\n",
    "    mask = cv2.inRange(hsv_img, lower_pink, upper_pink)\n",
    "    result = cv2.bitwise_and(img, img, mask=mask)\n",
    "    return result\n",
    "\n",
    "def match_gestures(handedness, img2, threshold=110):\n",
    "    print(handedness)\n",
    "    img2_processed = preprocess_image(img2)\n",
    "    orb = cv2.ORB_create()\n",
    "    \n",
    "    for val in gestures:\n",
    "        des1_list = data[val]\n",
    "        des1 = np.array(des1_list)\n",
    "        kp2, des2 = orb.detectAndCompute(img2_processed, None)\n",
    "\n",
    "        if des1 is not None and des2 is not None and len(des1) > 0 and len(des2) > 0:\n",
    "            if des1.dtype != np.uint8:\n",
    "                des1 = des1.astype(np.uint8)\n",
    "            if des2.dtype != np.uint8:\n",
    "                des2 = des2.astype(np.uint8)\n",
    "\n",
    "            bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "            matches = bf.match(des1, des2)\n",
    "            matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "            if len(matches) > threshold:\n",
    "                print(\"The gestures are similar.\")\n",
    "                if val == \"to_right\" and handedness == \"Right\":\n",
    "                    return \"to_left\"\n",
    "                elif val == \"to_left\" and handedness == \"Left\":\n",
    "                    return \"to_right\"\n",
    "                else:\n",
    "                    return val\n",
    "        else:\n",
    "            print(\"One or both sets of descriptors are missing or empty.\")\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 (no detections), 256.0ms\n",
      "Speed: 3.3ms preprocess, 256.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 myright, 92.7ms\n",
      "Speed: 1.4ms preprocess, 92.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 90.7ms\n",
      "Speed: 1.4ms preprocess, 90.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 90.7ms\n",
      "Speed: 1.5ms preprocess, 90.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 153.8ms\n",
      "Speed: 1.7ms preprocess, 153.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 150.6ms\n",
      "Speed: 2.6ms preprocess, 150.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 139.2ms\n",
      "Speed: 1.6ms preprocess, 139.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 81.4ms\n",
      "Speed: 1.5ms preprocess, 81.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 83.4ms\n",
      "Speed: 1.4ms preprocess, 83.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 97.3ms\n",
      "Speed: 1.5ms preprocess, 97.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myright, 117.5ms\n",
      "Speed: 1.5ms preprocess, 117.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 (no detections), 96.6ms\n",
      "Speed: 1.8ms preprocess, 96.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 myright, 139.2ms\n",
      "Speed: 2.0ms preprocess, 139.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "One or both sets of descriptors are missing or empty.\n",
      "\n",
      "0: 384x640 1 myleft, 1 myright, 188.9ms\n",
      "Speed: 2.7ms preprocess, 188.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "The gestures are similar.\n",
      "Right\n",
      "\n",
      "0: 384x640 1 myright, 173.0ms\n",
      "Speed: 5.0ms preprocess, 173.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 89.5ms\n",
      "Speed: 1.7ms preprocess, 89.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 109.4ms\n",
      "Speed: 1.5ms preprocess, 109.4ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 114.9ms\n",
      "Speed: 1.5ms preprocess, 114.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 99.2ms\n",
      "Speed: 1.4ms preprocess, 99.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 115.5ms\n",
      "Speed: 3.7ms preprocess, 115.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "The gestures are similar.\n",
      "\n",
      "0: 384x640 1 myright, 105.5ms\n",
      "Speed: 2.4ms preprocess, 105.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "\n",
      "0: 384x640 1 myright, 109.3ms\n",
      "Speed: 1.6ms preprocess, 109.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "\n",
      "0: 384x640 1 myright, 106.1ms\n",
      "Speed: 2.3ms preprocess, 106.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "\n",
      "0: 384x640 1 myright, 1 yourleft, 105.6ms\n",
      "Speed: 1.9ms preprocess, 105.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "Right\n",
      "\n",
      "0: 384x640 1 myright, 102.2ms\n",
      "Speed: 2.0ms preprocess, 102.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "\n",
      "0: 384x640 1 myright, 128.4ms\n",
      "Speed: 1.5ms preprocess, 128.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "\n",
      "0: 384x640 1 myright, 108.8ms\n",
      "Speed: 1.5ms preprocess, 108.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "\n",
      "0: 384x640 1 myright, 112.2ms\n",
      "Speed: 1.6ms preprocess, 112.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "\n",
      "0: 384x640 1 myright, 158.8ms\n",
      "Speed: 1.5ms preprocess, 158.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "\n",
      "0: 384x640 1 myright, 1 yourleft, 111.7ms\n",
      "Speed: 2.4ms preprocess, 111.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "The gestures are similar.\n",
      "Right\n",
      "\n",
      "0: 384x640 1 myright, 1 yourleft, 116.6ms\n",
      "Speed: 1.6ms preprocess, 116.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Right\n",
      "Right\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(1) \n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  \n",
    "\n",
    "        #image loading\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.flip(image, 1)\n",
    "        results = hands.process(image)\n",
    "        canvas_for_yolo = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        h, w, c = frame.shape\n",
    "\n",
    "        #for drawing\n",
    "        canvas = np.zeros_like(image)\n",
    "\n",
    "        #draw results on black canvas\n",
    "        if results.multi_hand_landmarks:\n",
    "            for handLMs in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    canvas, handLMs, mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                    mp_drawing.DrawingSpec(color=(250, 44, 250), thickness=2, circle_radius=2)\n",
    "                )\n",
    "            handedness = results.multi_handedness[0].classification[0].label\n",
    "    \n",
    "\n",
    "        width = canvas_for_yolo.shape[1]\n",
    "\n",
    "        #perform YOLO predictions\n",
    "        yolo_results = yolo_model.predict(frame)\n",
    "        for r in yolo_results:\n",
    "            #annotate boxes\n",
    "            annotator = Annotator(canvas)\n",
    "            boxes = r.boxes\n",
    "            for box in boxes:\n",
    "                b = box.xyxy[0]\n",
    "                mirrored_x_min = width - b[2]\n",
    "                mirrored_x_max = width - b[0]\n",
    "                mirrored_box = [mirrored_x_min-padding, b[1]-padding, mirrored_x_max+padding, b[3]+padding]\n",
    "                c = box.cls\n",
    "\n",
    "                #create cropped canvas for saving\n",
    "                save_me = canvas[max(int(mirrored_box[1]), 0):min(int(mirrored_box[3]), canvas.shape[0]),\n",
    "                               max(int(mirrored_box[0]), 0):min(int(mirrored_box[2]), canvas.shape[1])]\n",
    "\n",
    "                h, w = save_me.shape[:2]\n",
    "                current_aspect_ratio = w / h\n",
    "        \n",
    "                # Calculate padding\n",
    "                if current_aspect_ratio < desired_aspect_ratio:\n",
    "                    # Pad sides\n",
    "                    new_width = int(desired_aspect_ratio * h)\n",
    "                    pad_width = (new_width - w) // 2\n",
    "                    padded_image = cv2.copyMakeBorder(save_me, 0, 0, pad_width, pad_width, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "                else:\n",
    "                    # Pad top and bottom\n",
    "                    new_height = int(w / desired_aspect_ratio)\n",
    "                    pad_height = (new_height - h) // 2\n",
    "                    padded_image = cv2.copyMakeBorder(save_me, pad_height, pad_height, 0, 0, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "        \n",
    "                # Resize image to standard size\n",
    "                resized_image = cv2.resize(padded_image, standard_size, interpolation=cv2.INTER_AREA)\n",
    "                base_img  = cv2.imread(\"base_gestures/start_base.png\")\n",
    "                start = match_gestures(handedness, resized_image)\n",
    "                \n",
    "                if len(start) != 0:\n",
    "                    cv2.putText(canvas, start, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "                cv2.putText(canvas, \"Hand: \" + handedness, (50, 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "                if cv2.waitKey(10) & 0xFF == ord('s'):\n",
    "                    img_name = f\"cropped_hand_{img_counter}.png\"\n",
    "                    cv2.imwrite(\"saved_imgs/\" + img_name, resized_image)\n",
    "                    print(f\"{img_name} saved.\")\n",
    "                    img_counter += 1\n",
    "\n",
    "        #display\n",
    "        cv2.imshow('Hand Skeleton', canvas)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
